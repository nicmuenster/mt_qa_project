{"cells":[{"cell_type":"markdown","metadata":{"id":"xgEj-pZDNMXp"},"source":["Setting everything up"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28870,"status":"ok","timestamp":1654006049390,"user":{"displayName":"Nic Muenster","userId":"16548902816238615875"},"user_tz":-120},"id":"OMXQKwATM5fZ","outputId":"b61a85b5-60e7-441c-c05f-651da0631eba"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# mount to google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UFFIIxCONXDy"},"outputs":[],"source":["# necessary pip installs\n","!pip install Whoosh\n","!pip install transformers\n","!pip install sentencepiece\n","!pip install transformers[sentencepiece]\n","!pip install fasttext\n","!pip install pycountry\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ud16ZaCVPiaW"},"outputs":[],"source":["# import all necessary libraries\n","import codecs \n","from IPython.core.display import display, HTML\n","from whoosh.index import * \n","from whoosh.fields import *\n","from whoosh import qparser\n","import glob\n","import random\n","from transformers import BertForQuestionAnswering\n","import torch\n","from transformers import BertTokenizer\n","from transformers import pipeline\n","import fasttext\n","from pycountry import languages\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fke7gx7l6mLA"},"outputs":[],"source":["# setup the data, that shall be used as a knowledge base\n","path='drive/My Drive/Colab Notebooks/nlp-appl2-project/data/' #set path to passages\n","corpora = 'full_qa_corpora.txt'\n","# for poster, use of different sets possible, more data, more computation, always tradeoff \n","count=0\n","passages=[]\n","with open(path+corpora) as f:\n","    for line in f:\n","      count+=1\n","      passages.append(line)\n","      if count == 15000:\n","          pass\n","          #break\n","      #print(line)"]},{"cell_type":"markdown","metadata":{"id":"XwibRjHfVBYM"},"source":["Module 1: The Language Identification Module"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gjh2OtYyVF2J"},"outputs":[],"source":["class FastTextIdentifier(object):\n","    def __init__(self, file_path, k=5):\n","        self.identifier = fasttext.load_model(file_path)\n","        self.k = k\n","\n","    def predict_language(self, language_sample, verbose=False):\n","        forbidden_lang_list = ['als', 'arz', 'ast', 'azb', 'bar', 'bcl', 'bh', 'bpy', 'bxr', 'cbk', 'ceb', 'ckb', 'diq', 'dsb', 'dty', 'eml', 'frr', 'gom', 'hif', 'hsb', 'ilo', 'jbo', 'krc', 'lez', 'lmo', 'lrc', 'mai', 'mhr', 'min', 'mrj', 'mwl', 'myv', 'mzn', 'nah', 'nap', 'nds', 'new', 'pam', 'pfl', 'pms', 'pnb', 'rue', 'sah', 'scn', 'sco', 'tyv', 'vec', 'vep', 'vls', 'war', 'wuu', 'xal', 'xmf', 'yue']\n","        predictions, pred_score = self.identifier.predict(language_sample, k=self.k if verbose else 2)\n","        short_list = [prediction.split(\"__\")[-1] for prediction in predictions]\n","        index_list = []\n","        list_it = short_list.copy()\n","        for i, j in enumerate(list_it):\n","            if j in forbidden_lang_list:\n","                pass\n","            else:\n","                index_list.append(i)\n","        short_list = [short_list[i] for i in index_list]\n","        if short_list is None:\n","            success = False\n","        else:\n","            success = True\n","        if verbose and success:\n","            name_list = [languages.get(alpha_2=lang).name for lang in short_list]\n","            for i, language in enumerate(name_list):\n","                print(\"{order}. language_detected: {lang} wit a score of {score}\".format(order=i+1, lang=language, score=pred_score[i]))\n","        \n","        return short_list[0], pred_score[0], success\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":824,"status":"ok","timestamp":1654005432306,"user":{"displayName":"Nic Muenster","userId":"16548902816238615875"},"user_tz":-120},"id":"Y4BlImhNdihf","outputId":"8112f2db-8c46-47a9-e03b-04eae4cebb67"},"outputs":[{"name":"stdout","output_type":"stream","text":["['als', 'arz', 'ast', 'azb', 'bar', 'bcl', 'bh', 'bpy', 'bxr', 'cbk', 'ceb', 'ckb', 'diq', 'dsb', 'dty', 'eml', 'frr', 'gom', 'hif', 'hsb', 'ilo', 'jbo', 'krc', 'lez', 'lmo', 'lrc', 'mai', 'mhr', 'min', 'mrj', 'mwl', 'myv', 'mzn', 'nah', 'nap', 'nds', 'new', 'pam', 'pfl', 'pms', 'pnb', 'rue', 'sah', 'scn', 'sco', 'tyv', 'vec', 'vep', 'vls', 'war', 'wuu', 'xal', 'xmf', 'yue']\n"]}],"source":["# get correct languages from fasttext\n","forbidden_lang = []\n","lang_string = \"af als am an ar arz as ast av az azb ba bar bcl be bg bh bn bo bpy br bs bxr ca cbk ce ceb ckb co cs cv cy da de diq dsb dty dv el eml en eo es et eu fa fi fr frr fy ga gd gl gn gom gu gv he hi hif hr hsb ht hu hy ia id ie ilo io is it ja jbo jv ka kk km kn ko krc ku kv kw ky la lb lez li lmo lo lrc lt lv mai mg mhr min mk ml mn mr mrj ms mt mwl my myv mzn nah nap nds ne new nl nn no oc or os pa pam pfl pl pms pnb ps pt qu rm ro ru rue sa sah sc scn sco sd sh si sk sl so sq sr su sv sw ta te tg th tk tl tr tt tyv ug uk ur uz vec vep vi vls vo wa war wuu xal xmf yi yo yue zh\"\n","lang_list =lang_string.split(\" \")\n","for lang in lang_list:\n","    curr_lang = languages.get(alpha_2=lang)\n","    if curr_lang is None:\n","        forbidden_lang.append(lang)\n","print(forbidden_lang)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"10vEStbfFSCW"},"outputs":[],"source":["# sanity check of language identification model\n","alt_model_path = 'drive/My Drive/Colab Notebooks/nlp-appl2-project/models/lid.176.ftz'\n","model_path = 'drive/My Drive/Colab Notebooks/nlp-appl2-project/models/lid.176.bin' # big model works a lot better\n","fasttxt = FastTextIdentifier(model_path, k=6)\n","example1 = \"Ea orain funtzionatzen duen.\"\n","example2 = \"Hoffentlich klappt das jetzt\"\n","example3 = \"Je mange la baguette\"\n","result_lang1, score1, success = fasttxt.predict_language(example1, verbose=True)\n","print(result_lang1, score1)\n","result_lang2, score2, success = fasttxt.predict_language(example2, verbose=True)\n","print(result_lang2, score2)\n","result_lang3, score3, success = fasttxt.predict_language(example2, verbose=True)\n","print(result_lang3, score3)\n","result_lang4, score4, success = fasttxt.predict_language(example3, verbose=True)\n","print(result_lang4, score4)"]},{"cell_type":"markdown","metadata":{"id":"6xdP0nQRVHA0"},"source":["Module 2: The two-way Translation Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o5X1NPneYDBc"},"outputs":[],"source":["class TwoWayTranslator(object):\n","    def __init__(self):\n","        self.language_list = []\n","        self.unknown_language_list = []\n","        self.pipeline_dict = {}\n","        self.back_pipeline_dict = {}\n","\n","\n","    def get_language_model(self, source_language):\n","        success = True\n","        # prepare parameters\n","        model = \"Helsinki-NLP/opus-mt-{src}-{dst}\".format(src = source_language, dst = \"en\")\n","        model_back = \"Helsinki-NLP/opus-mt-{src}-{dst}\".format(src = \"en\", dst = source_language)\n","        translation_direction = \"translation_{src}_to_{dst}\".format(src = source_language, dst = \"en\")\n","        translation_direction_back = \"translation_{src}_to_{dst}\".format(src = \"en\", dst = source_language)\n","        # try to get translation pipelines\n","        try:\n","            translator  = pipeline(translation_direction, model=model, tokenizer=model)\n","            back_translator  = pipeline(translation_direction_back, model=model_back, tokenizer=model_back)\n","        except:\n","            success = False\n","            self.unknown_language_list.append(source_language)\n","        else:\n","            self.pipeline_dict[source_language] = translator\n","            self.back_pipeline_dict[source_language] = back_translator\n","            self.language_list.append(source_language)\n","        finally:\n","            return success\n","\n","\n","    def translate(self, text, source_language, to_english):\n","        if source_language in self.unknown_language_list:\n","            return \"Translation was not possible, no Language Model exists for this Language\", False\n","        if source_language not in self.language_list:\n","            success = self.get_language_model(source_language)\n","        else:\n","            success = True\n","        if not success:\n","            return \"Translation was not possible, no Language Model exists for this Language\", False\n","        else:\n","            if to_english:\n","                return self.pipeline_dict[source_language](text)[0][\"translation_text\"], True\n","            else: \n","                return self.back_pipeline_dict[source_language](text)[0][\"translation_text\"], True\n","\n","\n","\n","\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1PqTdOmH6s64"},"outputs":[],"source":["# sanity check for translation pipeline\n","two_way_translator = TwoWayTranslator()\n","src = \"de\"\n","dst = \"fr\"\n","example1 = \"Hoffentlich klappt das jetzt\"\n","\n","output1, _ = two_way_translator.translate(example1, src, to_english=True)\n","print(output1)\n","output2 = two_way_translator.translate(output1, dst, to_english=False)\n","output22 = two_way_translator.translate(output1, dst, to_english=False)\n","print(output2)\n","output3 = two_way_translator.translate(output2[0], dst, to_english=True)\n","print(output3)\n","output4 = two_way_translator.translate(output3[0], \"eus\", to_english=False)\n","print(output4)\n","output4 = two_way_translator.translate(output3[0], \"eu\", to_english=False)\n","print(output4)\n"]},{"cell_type":"markdown","metadata":{"id":"DUNNbgoHNBiD"},"source":["Module 3:\n","The QA System"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xHualDPGNH4o"},"outputs":[],"source":["# class definition of submodels\n","class IR(object):\n","    def __init__(self, passages):\n","        schema = Schema(id = ID(stored=True,unique=True),\n","                        text = TEXT(analyzer=analysis.StemmingAnalyzer())\n","                        )\n","        self.passages = passages\n","        if not os.path.exists(\"index\"):\n","              os.mkdir(\"index\")\n","        ix = create_in(\"index\", schema)\n","        writer = ix.writer() \n","        for ind,passage_text in enumerate(self.passages): \n","            writer.add_document(id=str(ind),text=passage_text)\n","        writer.commit()\n","        self.ix = ix\n","\n","    def retrieve_documents(self, query,topk):\n","        scores=[]\n","        text=[]\n","        with self.ix.searcher() as searcher:\n","            searcher = self.ix.searcher()\n","            q = qparser.QueryParser(\"text\", self.ix.schema, group=qparser.OrGroup).parse(query)\n","            results = searcher.search(q, limit=topk)\n","        for hit in results:\n","            scores.append(hit.score)\n","            text.append(self.passages[int(hit['id'])])\n","        return text, scores\n","\n","class QA(object):\n","    def __init__(self):       \n","        self.model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n","\n","    def answer_question(self, query,passage):\n","        input_ids = self.tokenizer.encode(query,passage) \n","        tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n","        sep_index = input_ids.index(self.tokenizer.sep_token_id)\n","        num_seg_a = sep_index + 1\n","        num_seg_b = len(input_ids) - num_seg_a\n","        segment_ids = [0]*num_seg_a + [1]*num_seg_b\n","        outputs = self.model(torch.tensor([input_ids]), \n","                             token_type_ids=torch.tensor([segment_ids]),\n","                             return_dict=True) \n","        start_scores = outputs.start_logits[0]\n","        end_scores = outputs.end_logits[0]\n","        max_score = -float('inf')\n","        index_start = None\n","        index_end = None\n","        # get best combination of indices\n","        for i in range(len(start_scores[num_seg_a:])):\n","            for j in range(len(end_scores[num_seg_a + i:-1])):\n","              current_score = start_scores[num_seg_a + i] + end_scores[num_seg_a + i + j]\n","              if current_score >= max_score:\n","                  max_score = current_score\n","                  index_start = num_seg_a + i\n","                  index_end = num_seg_a + i + j\n","        answer_start = index_start\n","        answer_end = index_end\n","        answer = tokens[answer_start]\n","\n","        for i in range(answer_start+1, answer_end+1):          \n","            if tokens[i][0:2] == '##':\n","                answer += tokens[i][2:]           \n","            else:\n","                # filter out remaining seperators\n","                if tokens[i] == '[SEP]':\n","                  continue\n","                else: \n","                  answer += ' ' + tokens[i]\n","\n","        return 'Answer: \"' + answer + '\"', max_score.item()/2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"epWGmvVWQj_r"},"outputs":[],"source":["# Wrapper class for IR and QA modules\n","class QA_pipeline(object):\n","    def __init__(self, passages):\n","        self.ir = IR(passages)\n","        self.qa = QA()\n","\n","    def answer_question(self, query, topk_docs=3):\n","        topk_docs, scores=self.ir.retrieve_documents(query, topk_docs)\n","        return self.qa.answer_question(query,topk_docs[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aaXmTo7lSFrg"},"outputs":[],"source":["#sanity check\n","test_wrapper = QA_pipeline(passages)\n","query='Range of incubation periods for the disease in humans'\n","print(test_wrapper.answer_question(query, 3))"]},{"cell_type":"markdown","metadata":{"id":"OvTQgUj3lxq0"},"source":["Bringing it all together: The final Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8jJbljBuQWmV"},"outputs":[],"source":["# Wrapper class for IR and QA modules\n","class MT_QA_pipeline(object):\n","    def __init__(self, passages, identification_threshold=0.70):\n","        assert passages is not None\n","        model_path = 'drive/My Drive/Colab Notebooks/nlp-appl2-project/models/lid.176.bin' # big model works a lot better\n","        self.fasttxt = FastTextIdentifier(model_path, k=6)\n","        self.qa_system = QA_pipeline(passages)\n","        self.two_way_translator = TwoWayTranslator()\n","        self.thresh = identification_threshold\n","\n","    def answer_question(self, query, topk_docs=1, verbose=False):\n","        source_language, score, success = self.fasttxt.predict_language(query, verbose=verbose)\n","        if score < self.thresh:\n","            return \"Language could not be identified for certain, Please try a different language or mean of phrasing it\"\n","        if source_language == 'en':\n","            english_question = query\n","        else:\n","            english_question, success = self.two_way_translator.translate(query, source_language, to_english=True)\n","            if verbose:\n","              print(english_question)\n","            if not success:\n","                return english_question\n","\n","        english_answer, _ = self.qa_system.answer_question(english_question, topk_docs=topk_docs)\n","        if verbose:\n","            print(english_answer)\n","        if source_language == 'en':\n","            answer = english_answer\n","        else:\n","            answer, _ = self.two_way_translator.translate(english_answer, source_language, to_english=False)\n","        return answer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1773107,"status":"ok","timestamp":1654011900516,"user":{"displayName":"Nic Muenster","userId":"16548902816238615875"},"user_tz":-120},"id":"dkaMyK2xbn-J","outputId":"16dca7d2-230a-40ed-fd14-0c2a53a7165d"},"outputs":[{"name":"stderr","output_type":"stream","text":["Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"]}],"source":["qa_mt_module=MT_QA_pipeline(passages)\n","# full passages take around 45 min to load"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"80CIAdLbufsk"},"outputs":[],"source":["while True:\n","    question= input()\n","    if question == \"stop\":\n","      break\n","    print(qa_mt_module.answer_question(question))\n","# What are common symptoms of Covid19?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O-k5G-tgvWfk"},"outputs":[],"source":["qa_mt_module=MT_QA_pipeline(passages)\n","while True:\n","    question= input()\n","    if question == \"stop\":\n","      break\n","    print(qa_mt_module.answer_question(question, 5, True))"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"multilingual_QA.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}