{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"online_wikipedia_mt_qa.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPOQs4LPYKT0zoS6BFcfqNH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# mount to google drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iV778k4U7Rmi","executionInfo":{"status":"ok","timestamp":1654181976391,"user_tz":-120,"elapsed":31852,"user":{"displayName":"Nic Muenster","userId":"16548902816238615875"}},"outputId":"b515b910-1fe1-4976-fd18-1c60382a6ff3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eCT2mB8mxrjw"},"outputs":[],"source":["# install necessary libraries\n","!pip install wikipedia\n","!pip install Whoosh\n","!pip install transformers\n","!pip install sentencepiece\n","!pip install transformers[sentencepiece]\n","!pip install fasttext\n","!pip install pycountry\n","!pip install sacremoses\n"]},{"cell_type":"code","source":["# import all needed libraries\n","import wikipedia\n","import codecs \n","from IPython.core.display import display, HTML\n","from whoosh.index import * \n","from whoosh.fields import *\n","from whoosh import qparser\n","import glob\n","import random\n","from transformers import BertForQuestionAnswering\n","import torch\n","from transformers import BertTokenizer\n","from transformers import pipeline\n","import fasttext\n","from pycountry import languages\n","import spacy"],"metadata":{"id":"ZmTu0ZIux95G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# first attempts of combining IR and the Wikipedia Search, with intial tests\n","class IR(object):\n","    def __init__(self, passages):\n","        schema = Schema(id = ID(stored=True,unique=True),\n","                        text = TEXT(analyzer=analysis.StemmingAnalyzer())\n","                        )\n","        self.passages = passages\n","        if not os.path.exists(\"index\"):\n","              os.mkdir(\"index\")\n","        ix = create_in(\"index\", schema)\n","        writer = ix.writer()\n","        for ind,passage_text in enumerate(self.passages): \n","            writer.add_document(id=str(ind),text=passage_text)\n","        writer.commit()\n","        self.ix = ix\n","        \n","\n","    def retrieve_documents(self, query,topk):\n","        scores=[]\n","        text=[]\n","        with self.ix.searcher() as searcher:\n","            searcher = self.ix.searcher()\n","            q = qparser.QueryParser(\"text\", self.ix.schema, group=qparser.OrGroup).parse(query)\n","            results = searcher.search(q, limit=topk)\n","        for hit in results:\n","            scores.append(hit.score)\n","            text.append(self.passages[int(hit['id'])])\n","        return text, scores\n","\n","def get_article(question):\n","  candidates = wikipedia.search(question)\n","  searcher = IR(candidates)\n","  return searcher.retrieve_documents(question,1)\n","\n","print(get_article(\"Who was george washington?\"))\n","print(get_article(\"Who is Barack Obama?\"))\n","print(get_article(\"What frequency has visible light?\"))\n","print(wikipedia.summary(get_article(\"What frequency has visible light?\")[0][0]))"],"metadata":{"id":"yhncQnHOzwrt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# spacy test stuff\n","nlp = spacy.load(\"en_core_web_sm\")\n","nlp.get_pipe(\"ner\").labels\n","doc = nlp(u\"Where is spain\")\n","for ent in doc.ents:\n","    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n","doc = nlp(u\"What is NLP?\")\n","for ent in doc.ents:\n","    print(ent.text, ent.start_char, ent.end_char, ent.label_)\n","doc = nlp(u\"What is the BERT language model?\")\n","for ent in doc.ents:\n","    print(ent.text, ent.start_char, ent.end_char, ent.label_)   \n","doc = nlp(u\"Who was George Washington and what did he have to do with elvis presley?\")\n","for ent in doc.ents:\n","    print(ent.text, ent.start_char, ent.end_char, ent.label_)"],"metadata":{"id":"8stAvfErIW2f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# All necessary modules and submodules\n","\n","# informantion retrival module\n","class IR(object):\n","    def __init__(self, passages):\n","        schema = Schema(id = ID(stored=True,unique=True),\n","                        text = TEXT(analyzer=analysis.StemmingAnalyzer())\n","                        )\n","        self.passages = passages\n","        if not os.path.exists(\"index\"):\n","              os.mkdir(\"index\")\n","        ix = create_in(\"index\", schema)\n","        writer = ix.writer() #run once! or restart runtime\n","        for ind,passage_text in enumerate(self.passages): \n","            writer.add_document(id=str(ind),text=passage_text)\n","        writer.commit()\n","        self.ix = ix\n","        \n","\n","    def retrieve_documents(self, query,topk):\n","        scores=[]\n","        text=[]\n","        with self.ix.searcher() as searcher:\n","            searcher = self.ix.searcher()\n","            q = qparser.QueryParser(\"text\", self.ix.schema, group=qparser.OrGroup).parse(query)\n","            results = searcher.search(q, limit=topk)\n","        for hit in results:\n","            scores.append(hit.score)\n","            text.append(self.passages[int(hit['id'])])\n","        return text, scores\n","\n","# question answering module\n","class QA(object):\n","    def __init__(self):       \n","        self.model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n","        self.tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n","\n","    def answer_question(self, query,passage):\n","        # encode inputs, only use first 512 tokens, since the BERT transformer is limited\n","        input_ids = self.tokenizer.encode(query,passage)[:512]\n","        tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n","        sep_index = input_ids.index(self.tokenizer.sep_token_id)\n","        num_seg_a = sep_index + 1\n","        num_seg_b = len(input_ids) - num_seg_a\n","        segment_ids = [0]*num_seg_a + [1]*num_seg_b\n","        outputs = self.model(torch.tensor([input_ids]), \n","                             token_type_ids=torch.tensor([segment_ids]),\n","                             return_dict=True) \n","        # extract scores\n","        start_scores = outputs.start_logits[0][:512]\n","        end_scores = outputs.end_logits[0][:512]\n","        max_score = -float('inf')\n","        index_start = None\n","        index_end = None\n","        # get best combination of indices\n","        for i in range(len(start_scores[num_seg_a:])):\n","            for j in range(len(end_scores[num_seg_a + i:-1])):\n","              current_score = start_scores[num_seg_a + i] + end_scores[num_seg_a + i + j]\n","              if current_score >= max_score:\n","                  max_score = current_score\n","                  index_start = num_seg_a + i\n","                  index_end = num_seg_a + i + j\n","        answer_start = index_start\n","        answer_end = index_end\n","        answer = tokens[answer_start]\n","        # buid answer from tokens\n","        for i in range(answer_start+1, answer_end+1):          \n","            if tokens[i][0:2] == '##':\n","                answer += tokens[i][2:]           \n","            else:\n","                # filter out remaining seperators if necessary\n","                if tokens[i] == '[SEP]':\n","                  continue\n","                else: \n","                  answer += ' ' + tokens[i]\n","        # return answer plus confidence score\n","        return 'Answer: \"' + answer + '\"', max_score.item()/2\n","\n","# Wrapper class for IR and QA modules, combined with the wikipedia pipeline\n","class QA_pipeline(object):\n","    def __init__(self):        \n","        self.qa = QA()\n","        self.nlp = spacy.load(\"en_core_web_sm\")\n","\n","    def get_best_candidate(self, question, verbose=False):\n","        doc = self.nlp(question)\n","        # if an entity has been found, go down this path      \n","        if doc.ents:\n","          entity = doc.ents[0]\n","          if (entity.label_ == \"GPE\") or (entity.label_ == \"PERSON\") or(entity.label_ == \"ORG\"):\n","            candidate = entity.text\n","            if verbose:\n","              print(\"initial result of spacy: \" + candidate)\n","            # compare wikipedia search results with the found entity using the IR module\n","            candidates = wikipedia.search(candidate)\n","            searcher = IR(candidates)\n","            ordered_candidates = searcher.retrieve_documents(question,1)\n","            if verbose:\n","              print(candidates)\n","              print(ordered_candidates)\n","            if ordered_candidates[0]:\n","              candidate = ordered_candidates[0][0]\n","            else:\n","              candidate = candidates[0]\n","            if verbose:\n","              print(\"result after retrieval: \" + candidate)\n","            try:\n","              passage = wikipedia.summary(candidate)         \n","            except:\n","              return \"There is no Information on this topic available, please ask something else\", False\n","            else:\n","              return passage, True\n","        # otherwise compare wikipedia search results with the question using the IR module\n","        candidates = wikipedia.search(question)\n","        searcher = IR(candidates)\n","        ordered_candidates = searcher.retrieve_documents(question,1)\n","        if verbose:\n","          print(candidates)\n","          print(ordered_candidates)\n","        if ordered_candidates[0]:\n","          candidate = ordered_candidates[0][0]\n","        else:\n","          candidate = candidates[0]\n","        try:\n","            passage = wikipedia.summary(candidate)\n","        except:\n","            return \"There is no Information on this topic available, please ask something else\", False\n","        else:\n","            if verbose:\n","              print(passage)\n","            return passage, True\n","\n","    def answer_question(self, query, verbose=False):\n","        passage, success = self.get_best_candidate(query, verbose=verbose)\n","        if success:\n","          return self.qa.answer_question(query, passage)\n","        else:\n","          return passage, 0"],"metadata":{"id":"ho1MiRTo4lLK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# language identification and translation components as well as final wrapper class\n","class FastTextIdentifier(object):\n","    def __init__(self, file_path, k=5):\n","        # load fasttext model and define a k\n","        self.identifier = fasttext.load_model(file_path)\n","        self.k = k\n","\n","    def predict_language(self, language_sample, verbose=False):\n","        # filter out languages in wrong ISO format\n","        forbidden_lang_list = ['als', 'arz', 'ast', 'azb', 'bar', 'bcl', 'bh', 'bpy', 'bxr', 'cbk', 'ceb', 'ckb', 'diq', 'dsb', 'dty', 'eml', 'frr', 'gom', 'hif', 'hsb', 'ilo', 'jbo', 'krc', 'lez', 'lmo', 'lrc', 'mai', 'mhr', 'min', 'mrj', 'mwl', 'myv', 'mzn', 'nah', 'nap', 'nds', 'new', 'pam', 'pfl', 'pms', 'pnb', 'rue', 'sah', 'scn', 'sco', 'tyv', 'vec', 'vep', 'vls', 'war', 'wuu', 'xal', 'xmf', 'yue']\n","        predictions, pred_score = self.identifier.predict(language_sample, k=self.k if verbose else 2)\n","        short_list = [prediction.split(\"__\")[-1] for prediction in predictions]\n","        index_list = []\n","        # filter trough candidates\n","        list_it = short_list.copy()\n","        for i, j in enumerate(list_it):\n","            if j in forbidden_lang_list:\n","                pass\n","            else:\n","                index_list.append(i)\n","        short_list = [short_list[i] for i in index_list]\n","        if short_list is None:\n","            success = False\n","        else:\n","            success = True\n","        if verbose and success:\n","            name_list = [languages.get(alpha_2=lang).name for lang in short_list]\n","            for i, language in enumerate(name_list):\n","                print(\"{order}. language_detected: {lang} wit a score of {score}\".format(order=i+1, lang=language, score=pred_score[i]))\n","        \n","        return short_list[0], pred_score[0], success\n","\n","\n","\n","class TwoWayTranslator(object):\n","    def __init__(self):\n","        self.language_list = []\n","        self.unknown_language_list = []\n","        self.pipeline_dict = {}\n","        self.back_pipeline_dict = {}\n","\n","\n","    def get_language_model(self, source_language):\n","        success = True\n","        # prepare parameters\n","        model = \"Helsinki-NLP/opus-mt-{src}-{dst}\".format(src = source_language, dst = \"en\")\n","        model_back = \"Helsinki-NLP/opus-mt-{src}-{dst}\".format(src = \"en\", dst = source_language)\n","        translation_direction = \"translation_{src}_to_{dst}\".format(src = source_language, dst = \"en\")\n","        translation_direction_back = \"translation_{src}_to_{dst}\".format(src = \"en\", dst = source_language)\n","        # try to get translation pipelines\n","        try:\n","            translator  = pipeline(translation_direction, model=model, tokenizer=model)\n","            back_translator  = pipeline(translation_direction_back, model=model_back, tokenizer=model_back)\n","        except:\n","            success = False\n","            self.unknown_language_list.append(source_language)\n","        else:\n","            self.pipeline_dict[source_language] = translator\n","            self.back_pipeline_dict[source_language] = back_translator\n","            self.language_list.append(source_language)\n","        finally:\n","            return success\n","\n","\n","    def translate(self, text, source_language, to_english):\n","        if source_language in self.unknown_language_list:\n","            return \"Translation was not possible, no Language Model exists for this Language\", False\n","        if source_language not in self.language_list:\n","            success = self.get_language_model(source_language)\n","        else:\n","            success = True\n","        if not success:\n","            return \"Translation was not possible, no Language Model exists for this Language\", False\n","        else:\n","            if to_english:\n","                return self.pipeline_dict[source_language](text)[0][\"translation_text\"], True\n","            else: \n","                return self.back_pipeline_dict[source_language](text)[0][\"translation_text\"], True\n","\n","\n","# final wrapper class containing all modules\n","class MT_QA_Pipeline(object):\n","    def __init__(self, identification_threshold=0.40):\n","        model_path = 'drive/My Drive/Colab Notebooks/nlp-appl2-project/models/lid.176.bin' # big model works a lot better\n","        self.fasttxt = FastTextIdentifier(model_path, k=6)\n","        self.qa_system = QA_pipeline()\n","        self.two_way_translator = TwoWayTranslator()\n","        self.thresh = identification_threshold\n","\n","    def answer_question(self, query, verbose=False):\n","        source_language, score, success = self.fasttxt.predict_language(query, verbose=verbose)\n","        if score < self.thresh:\n","            return \"Language could not be identified for certain, Please try a different language or mean of phrasing it\"\n","        if source_language == 'en':\n","            #print(\"success\")\n","            english_question = query\n","        else:\n","            english_question, success = self.two_way_translator.translate(query, source_language, to_english=True)\n","            if verbose:\n","              print(english_question)\n","            if not success:\n","                return english_question\n","\n","        english_answer, _ = self.qa_system.answer_question(english_question, verbose=verbose)\n","        if verbose:\n","            print(english_answer)\n","        if source_language == 'en':\n","            answer = english_answer\n","        else:\n","            answer, _ = self.two_way_translator.translate(english_answer, source_language, to_english=False)\n","        return answer"],"metadata":{"id":"K5UKL8eh6NHn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mt_qa_module=MT_QA_Pipeline()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s3nL0-Fx62uH","executionInfo":{"status":"ok","timestamp":1654186296171,"user_tz":-120,"elapsed":15292,"user":{"displayName":"Nic Muenster","userId":"16548902816238615875"}},"outputId":"9c4299c2-d3d2-4465-c0eb-d8f916fdc998"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"]}]},{"cell_type":"code","source":["# input loop for question input, can by stopped with the safe word \"stop\"\n","while True:\n","    question = input()\n","    if question == \"stop\":\n","      break\n","    print(mt_qa_module.answer_question(question))\n","# Who was George Washington?\n","# Who was Barack Obama?\n","# What frequency has visible light?\n","# When was the first car built? --> summary of car leads to cat for whatever reason lol\n","# What is NLP?\n","# What is the BERT language model?\n","# How many people live in london?\n","# How many people live in paris?\n","# How many people live in france?\n","# What language is spoken in france?\n","# What language is spoken in Nigeria?\n","# What are the symptoms of Covid19?\n","# what is Pernicious anemia?\n","# What language is spoken on the Bahamas?\n","# What frequency has visible light? \n","# Wer war George Washington?\n","# Wer ist Barack Obama?\n","# Wer ist Barack Hussein Obama II?\n","# Wo ist Barack Obama geboren worden?\n","# ¿Quien es Barack Obama?\n","# ¿Quien es Stephen King?\n","# ¿Que es la pelicula mas famosa de Steven Spielberg?\n","# ¿Que es el libro mas famoso de Hemmingway?\n","# Wann wurde Deutschland wiedervereinigt?\n","# Was ist der Sinn des Lebens?\n","# ¿Que es el sentido de la vida?"],"metadata":{"id":"1IphHJVw6-JT"},"execution_count":null,"outputs":[]}]}